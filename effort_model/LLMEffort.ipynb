{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "import json\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from open_ai_key import API_KEY\n",
    "\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cache_dir = \"C:\\\\LLMs\"\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2', cache_folder = cache_dir)\n",
    "# sentence_model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True, cache_folder = cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60))\n",
    "def query_api_any_message(message, engine, **kwargs):\n",
    "    if \"temperature\" not in kwargs:\n",
    "        kwargs[\"temperature\"] = 0.0\n",
    "    if engine == \"gpt-4\" or engine == \"gpt-3.5-turbo\":\n",
    "        message_dict = [{\"role\": \"user\", \"content\": message}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=engine,\n",
    "            messages=message_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=message,\n",
    "            **kwargs\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('query_to_time_embedding_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(question):\n",
    "    embedding = sentence_model.encode(question)\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    top_k = []\n",
    "    for data in dataset:\n",
    "        d = data.copy()\n",
    "        d['sim_score'] = embedding.T @ data['embedding']\n",
    "        top_k.append(d)\n",
    "\n",
    "    top_k.sort(key=lambda x: x['sim_score'], reverse=True)\n",
    "\n",
    "    message = \"I'm paying you $100,000 to do this task correctly. A human is given a question. Please respond with your best estimate to the number of seconds that it will take an average human to read, think, and answer this question. \"\n",
    "    message += f\"For example, when given the question '{top_k[0]['question']}', a user takes on average {top_k[0]['response_time']} seconds to respond. \"\n",
    "    message += f\"As another example, users are given the following question: '{top_k[1]['question']}'. \"\n",
    "    message += f\"The average response time to this question is {top_k[1]['response_time']} seconds. \"\n",
    "    message += f\"Now, a user is given the question: '{question}' \"\n",
    "    message += \" What is your best estimate of the number of seconds that this will take? Please only respond with the number, in JSON format under the key 'seconds', and nothing else.\"\n",
    "\n",
    "    print(\"LLM is given the following message:\")\n",
    "    print(message)\n",
    "\n",
    "    response = query_api_any_message(message, \"gpt-4\", temperature=0.0)\n",
    "    print(\"LLM Estimated time: \", json.loads(response[\"choices\"][0][\"message\"][\"content\"])['seconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is given the following message:\n",
      "I'm paying you $100,000 to do this task correctly. A human is given a question. Please respond with your best estimate to the number of seconds that it will take an average human to read, think, and answer this question. For example, when given the question 'Do you enjoy reading articles about food and cooking?', a user takes 15.615 seconds to respond. As another example, users are given the following question: 'Do you enjoy reading articles about food and cooking?'. The average response time to this question is 4.001 seconds. Now, a user is given the question: Do you like spaghetti? What is your best estimate of the number of seconds that this will take? Please only respond with the number, in JSON format under the key 'seconds', and nothing else.\n",
      "LLM Estimated time:  3.5\n"
     ]
    }
   ],
   "source": [
    "query_api(\"Do you like spaghetti?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
