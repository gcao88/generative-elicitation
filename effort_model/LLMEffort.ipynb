{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "import json\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from open_ai_key import API_KEY\n",
    "\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cache_dir = \"C:\\\\LLMs\"\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2', cache_folder = cache_dir)\n",
    "# sentence_model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True, cache_folder = cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60))\n",
    "def query_api_any_message(message, engine, **kwargs):\n",
    "    if \"temperature\" not in kwargs:\n",
    "        kwargs[\"temperature\"] = 0.0\n",
    "    if engine == \"gpt-4\" or engine == \"gpt-3.5-turbo\":\n",
    "        message_dict = [{\"role\": \"user\", \"content\": message}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=engine,\n",
    "            messages=message_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=message,\n",
    "            **kwargs\n",
    "        )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158\n"
     ]
    }
   ],
   "source": [
    "with open('query_to_time_embedding_dataset.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "l = len(dataset)\n",
    "test_len = round(0.1*l)\n",
    "test_dataset = dataset[-1*test_len:]\n",
    "\n",
    "dataset = dataset[0:-1*test_len]\n",
    "\n",
    "dataset_no_dupes = set([(data['question'], tuple(data['embedding']/np.linalg.norm(data['embedding']))) for data in dataset])\n",
    "dataset_no_dupes = list(dataset_no_dupes)\n",
    "dataset_no_dupes_cleaned = []\n",
    "for d in dataset_no_dupes:\n",
    "    data = {'question': d[0], 'embedding': np.array(d[1])}\n",
    "    all_times = []\n",
    "    for datapoint in dataset:\n",
    "        if datapoint['question'] == data['question']:\n",
    "            all_times.append(datapoint['response_time'])\n",
    "    data['response_time'] = np.mean(all_times)\n",
    "    data['stdev'] = np.std(all_times)\n",
    "    dataset_no_dupes_cleaned.append(data)\n",
    "\n",
    "print(len(dataset_no_dupes_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(question, verbose=False):\n",
    "    embedding = sentence_model.encode(question)\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    top_k = []\n",
    "    for data in dataset_no_dupes_cleaned:\n",
    "        d = data.copy()\n",
    "        d['sim_score'] = embedding.T @ d['embedding']\n",
    "        top_k.append(d)\n",
    "\n",
    "    top_k.sort(key=lambda x: x['sim_score'], reverse=True)\n",
    "\n",
    "    message = \"I'm paying you $100,000 to do this task correctly. A human is given a question. Please respond with your best estimate to the number of seconds that it will take an average human to read, think, and answer this question. \"\n",
    "    message += f\"For example, when given the question '{top_k[0]['question']}', a user takes on average {top_k[0]['response_time']} seconds to respond with a standard deviation of {top_k[0]['stdev']} seconds. \"\n",
    "    message += f\"As another example, users are given the following question: '{top_k[1]['question']}'. \"\n",
    "    message += f\"The average response time to this question is {top_k[1]['response_time']} seconds with a standard deviation of {top_k[1]['stdev']} seconds. \"\n",
    "    message += f\"Now, a user is given the question: '{question}' \"\n",
    "    message += \" What is your best estimate of the number of seconds that this will take? Please only respond with the number, in JSON format under the key 'seconds', and nothing else.\"\n",
    "\n",
    "    response = query_api_any_message(message, \"gpt-4\", temperature=0.0)\n",
    "    if verbose:\n",
    "        print(\"LLM is given the following message:\")\n",
    "        print(message)\n",
    "        print(\"LLM Estimated time: \", json.loads(response[\"choices\"][0][\"message\"][\"content\"])['seconds'])\n",
    "\n",
    "\n",
    "    return json.loads(response[\"choices\"][0][\"message\"][\"content\"])['seconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is given the following message:\n",
      "I'm paying you $100,000 to do this task correctly. A human is given a question. Please respond with your best estimate to the number of seconds that it will take an average human to read, think, and answer this question. For example, when given the question 'Do you enjoy reading articles about food and cooking?', a user takes on average 6.6248571428571426 seconds to respond with a standard deviation of 5.1027627370886455 seconds. As another example, users are given the following question: 'Do you like reading articles about food and cooking recipes?'. The average response time to this question is 2.721 seconds with a standard deviation of 0.0 seconds. Now, a user is given the question: 'Do you like spaghetti?'  What is your best estimate of the number of seconds that this will take? Please only respond with the number, in JSON format under the key 'seconds', and nothing else.\n",
      "LLM Estimated time:  2.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_api(\"Do you like spaghetti?\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'response_time', 'prolific_id', 'embedding'])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MSE: 338.36004851162517\n",
      "Accuracy: 0.4584717607973422\n"
     ]
    }
   ],
   "source": [
    "mse_loss = 0\n",
    "accurate = 0\n",
    "for data in test_dataset:\n",
    "    true = data['response_time']\n",
    "    pred = query_api(data['question'])\n",
    "    data['LLM_prediction'] = pred\n",
    "    mse_loss += (true - pred)**2\n",
    "    if abs(true - pred) < 5:\n",
    "        accurate += 1\n",
    "\n",
    "avg_mse_loss = mse_loss / len(test_dataset)\n",
    "print(\"Avg MSE:\", avg_mse_loss)\n",
    "print(\"Accuracy:\", accurate/len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6539313399778516\n"
     ]
    }
   ],
   "source": [
    "good = 0\n",
    "for i in range(len(test_dataset)):\n",
    "    for j in range(i+1, len(test_dataset)):\n",
    "        data1 = test_dataset[i]\n",
    "        data2 = test_dataset[j]\n",
    "        if (data1['LLM_prediction'] - data2['LLM_prediction'])*(data1['response_time'] - data2['response_time']) >= 0:\n",
    "            good += 1\n",
    "\n",
    "print(good/(len(test_dataset)*(len(test_dataset)-1)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LLM_test_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(test_dataset, file)\n",
    "\n",
    "with open('LLM_nearest_neighbors_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(dataset_no_dupes_cleaned, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the MLP Effort model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\georg\\\\OneDrive\\\\Documents\\\\George MIT\\\\UROP\\\\CLEAR\\\\github\\\\generative-elicitation\\\\effort_model', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312', '', 'C:\\\\Users\\\\georg\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\georg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\georg\\\\OneDrive\\\\Documents\\\\George MIT\\\\UROP\\\\CLEAR\\\\github\\\\generative-elicitation\\\\..', 'c:\\\\Users\\\\georg\\\\OneDrive\\\\Documents\\\\George MIT\\\\UROP\\\\CLEAR\\\\github\\\\generative-elicitation']\n",
      "768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_3032\\1480618310.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  effort_model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResponseTimePredictor(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython import get_ipython\n",
    "\n",
    "# print(sys.path)\n",
    "# notebook_path = os.path.abspath(get_ipython().get_ipython().magic('pwd'))\n",
    "# path = os.path.join(os.path.dirname(notebook_path), '..')\n",
    "# sys.path.append(os.path.dirname(notebook_path))\n",
    "\n",
    "from effort_model_class import ResponseTimePredictor\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "effort_model = ResponseTimePredictor(sentence_model.get_sentence_embedding_dimension())\n",
    "print(sentence_model.get_sentence_embedding_dimension())\n",
    "effort_model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
    "effort_model.to(device)\n",
    "effort_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg MSE: 184.50002308515695\n",
      "Accuracy: 0.45514950166112955\n"
     ]
    }
   ],
   "source": [
    "mse_loss = 0\n",
    "accurate = 0\n",
    "for data in test_dataset:\n",
    "    true = data['response_time']\n",
    "    embeddings = sentence_model.encode(data['question'], convert_to_tensor=True).to(device)\n",
    "    pred = effort_model(embeddings).item()\n",
    "    mse_loss += (true - pred)**2\n",
    "    if abs(true - pred) < 5:\n",
    "        accurate += 1\n",
    "\n",
    "avg_mse_loss = mse_loss / len(test_dataset)\n",
    "print(\"Avg MSE:\", avg_mse_loss)\n",
    "print(\"Accuracy:\", accurate/len(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
